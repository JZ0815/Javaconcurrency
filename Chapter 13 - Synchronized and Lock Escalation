## Chapter 13 Synchronized and Lock Escalation
### **13.1 Synchronized Lock Optimization Roadmap**

#### **13.1.1 Background and Motivation**
The fundamental trade-off in concurrent programming:
- **Safety vs. Performance**
  - *Using locks*: Ensures thread safety but reduces performance due to contention
  - *Lock-free approaches*: Maximize parallelism but risk data integrity issues
- **The Optimization Challenge**:  
  Find the equilibrium where safety requirements are met with minimal performance overhead.

#### **13.1.2 Synchronized Lock Mechanism**
Synchronized locks in Java leverage the **Mark Word** in object headers, which dynamically reuses its bits to represent different lock states through a progressive upgrade path:

1. **Lock State Transition Path**:
   ```
   No Lock → Biased Lock → Lightweight Lock → Heavyweight Lock
   ```

2. **Mark Word Adaptation**:
   - The 64-bit Mark Word repurposes its bits to store:
     - Lock flags (2 bits)
     - Thread ID (54 bits when biased)
     - Hashcode (when unlocked)
     - GC age (4 bits)
   - This bitwise flexibility enables lock state changes without structural modification

3. **Optimization Strategy**:
   - **Initial (No Lock)**: Minimal overhead for non-contended objects
   - **Biased Locking**: Optimizes for repeated access by single thread
   - **Lightweight Lock (Thin Lock)**: Uses CAS for short-duration contention
   - **Heavyweight Lock**: Falls back to OS-level monitor for severe contention

#### **13.1.3 Key Technical Points**
- **Zero-Cost Abstraction**: Lock state changes occur in-place within Mark Word
- **Progressive Cost**: Each escalation level increases overhead proportionally to actual contention
- **JVM Autonomy**: Runtime automatically selects appropriate lock level

**Visualization of Mark Word States**:
```
|-----------------------------------------------------------------|
| State        | Storage Contents                                |
|--------------|------------------------------------------------|
| No Lock      | Hashcode (31b) | Age (4b) | 0 | 01             |
| Biased Lock  | ThreadID (54b) | Epoch (2b) | 1 | 01           |
| Lightweight  | Pointer to Lock Record (62b) | 00               |
| Heavyweight  | Pointer to Monitor (62b) | 10                   |
|-----------------------------------------------------------------|
```

#### **13.1.4 Practical Implications**
1. **Performance Tuning**:
   - Monitor lock escalation patterns via JVM logs
   - Prefer biased locking for single-threaded access patterns

2. **Anti-Patterns**:
   - Premature synchronization on high-contention objects
   - Ignoring lock elimination opportunities (e.g., thread-local objects)

3. **Debugging**:
   - Use `jol` to inspect Mark Word changes
   - Analyze with `-XX:+PrintSynchronizationStatistics`

This systematic approach to lock optimization demonstrates Java's sophisticated balance between thread safety and runtime efficiency.

### **13.2 Evolution of Synchronized Performance**

#### **13.2.1 The Heavyweight Era (Pre-Java 5)**
In early Java versions, `synchronized` was implemented purely as an **OS-level heavyweight lock** with significant performance costs:

1. **Kernel Involvement**
   - Each lock operation required **user-to-kernel mode transitions**
   - Thread blocking/waking needed OS intervention via **mutex locks**

2. **Performance Bottlenecks**
   - Context switches between modes consumed more time than simple critical sections
   - State preservation during transitions (registers/memory) added overhead
   - For trivial synchronized blocks, the locking overhead could exceed execution time

3. **Architectural Impact**
   ```
   Java Thread → OS Native Thread → Mutex Lock
   (User Space)    (Kernel Space)    (System Call)
   ```

#### **13.2.2 Why Every Object Can Be a Lock**
The capability stems from **intrinsic design** in JVM:

1. **Monitor Integration**
   - Every Java object contains a **hidden monitor lock** (via Mark Word)
   - Defined in `markOop.hpp`: 
     ```cpp
     ObjectHeader → Mark Word → Lock Bits
     ```

2. **Monitor Mechanics**
   - When locked:
     - Mark Word's `LockWord` points to a **Monitor object**
     - Monitor's `Owner` field stores the thread ID
   - Underlying dependency on OS mutex (retained for heavyweight locks)

3. **Kernel Dependency Legacy**
   ``` 
   Java Monitor → OS Mutex Lock → CPU State Switch
   (High Cost)
   ```

#### **13.2.3 Modern Optimization (Java 6+)**
To minimize lock overhead, JVM introduced **tiered locking**:

1. **Progressive Upgrade Path**
   ```
   Biased Lock → Lightweight Lock → Heavyweight Lock
   (No Contention) (CAS-Based)    (OS Mutex)
   ```

2. **Key Improvements**
   - **Biased Locking**: Skips CAS for thread-local access
   - **Thin Locks**: Uses atomic compare-and-swap (CAS) instead of OS calls
   - **Adaptive Spinning**: Temporarily avoids kernel transitions

3. **Upgrade Conditions**
   | Lock State       | Trigger Condition                     | Overhead        |
   |------------------|---------------------------------------|-----------------|
   | **Biased**       | Single-thread access                  | ~1 cycle        |
   | **Lightweight**  | Moderate contention (CAS succeeds)    | ~10-100 cycles  |
   | **Heavyweight**  | High contention (CAS fails)           | >1000 cycles    |

4. **Design Philosophy**
   - **"Pay-as-you-go"**: Only escalate to costly locks when necessary
   - **Throughput Preservation**: 90%+ of locks avoid kernel mode in modern apps

**Performance Comparison**:
- Java 5: All sync → Kernel mode (~10,000 cycles)
- Java 6+: 95% stay in user mode (~10-100 cycles)

This evolution transformed `synchronized` from a performance liability to a **scalable synchronization primitive**.


### **13.3 Synchronized Lock Types and Upgrade Process**

#### **13.3.1 Thread Access Patterns**
1. **Single Thread Access**  
   - Only one thread accesses the synchronized block
2. **Two Threads Alternating Access**  
   - Threads A and B alternately enter the synchronized block
3. **High Contention**  
   - Multiple threads compete for the lock

#### **13.3.2 Lock Upgrade Mechanism**
The lock state is stored in the object's **Mark Word**, with transitions triggered by lock flag bits:

```
No Lock → Biased Lock → Lightweight Lock → Heavyweight Lock
```

#### **13.3.3 No Lock (Initial State)**
- **Characteristics**:  
  - Default state for new objects  
  - No thread competition exists  
- **Verification Code**:  
  ```java
  Object o = new Object();
  System.out.println(ClassLayout.parseInstance(o).toPrintable()); 
  ```
- **Mark Word Contents**:  
  - Hashcode (if calculated)  
  - GC age (4 bits)  
  - Lock flags: `001` (unlocked)  

#### **13.3.4 Biased Lock**
- **Purpose**: Optimize for single-thread repeated access  
- **Mechanism**:  
  1. First thread acquires lock via **CAS**, storing its ID in Mark Word  
  2. Subsequent entries skip synchronization checks  
- **Activation**:  
  ```bash
  -XX:+UseBiasedLocking -XX:BiasedLockingStartupDelay=0
  ```
- **Revocation**:  
  - Occurs when a second thread competes  
  - JVM pauses at **safepoint** to reset to lightweight lock  

#### **13.3.5 Lightweight Lock**
- **Trigger**: Moderate contention (CAS failures < threshold)  
- **Behavior**:  
  - Competing threads **spin** (adaptive in Java 6+)  
  - CAS attempts to update lock owner  
- **Key Settings**:  
  ```bash
  -XX:-UseBiasedLocking  # Skip biased, enter lightweight directly
  -XX:PreBlockSpin=10    # Legacy spin count (Java 5)
  ```
- **Vs Biased Lock**:  
  - Requires CAS on every entry/exit  
  - Releases lock immediately after sync block  

#### **13.3.6 Heavyweight Lock**
- **Conditions**:  
  - High contention (spin threshold exceeded)  
  - Long critical sections  
- **Implementation**:  
  - OS-level **mutex lock**  
  - Thread blocking via kernel scheduling  

#### **13.3.7 Upgrade Process Summary**
1. **Single Thread**:  
   - Biased lock (zero overhead after initial CAS)  
2. **Low Contention**:  
   - Lightweight lock (CAS + spinning)  
3. **High Contention**:  
   - Heavyweight lock (kernel mutex)  

**Performance Trade-offs**:  
| Lock Type      | CPU Cost | Scope                |  
|----------------|----------|----------------------|  
| Biased         | ~1ns     | Single-thread        |  
| Lightweight    | ~100ns   | Low contention       |  
| Heavyweight    | >10,000ns| High contention      |  

**Key Takeaways**:  
- Modern JVMs **auto-tune** lock upgrades  
- Prefer short synchronized blocks to avoid escalation  
- Monitor contention with tools like `jstack` and `JFR`  

This tiered approach balances thread safety with minimal performance overhead.

### **13.4 JIT Compiler Optimizations for Locks**

#### **13.4.1 JIT Compiler Overview**
The **Just-In-Time (JIT) Compiler** dynamically optimizes bytecode during runtime, including enhancements for `synchronized` operations. Key optimizations:


#### **13.4.2 Lock Elimination**
**Scenario**:  
When synchronization is provably redundant (no actual contention exists), the JIT removes lock operations entirely.

**Example**:  
```java
public class LockClearDemo {
    static Object sharedLock = new Object();  // Shared lock
    
    public void m1() {
        Object localObj = new Object();  // Local object (thread-confined)
        
        // Lock eliminated: 'localObj' cannot be shared across threads
        synchronized (localObj) {
            System.out.println("Lock eliminated for: " + localObj.hashCode());
        }
    }
    
    public static void main(String[] args) {
        LockClearDemo demo = new LockClearDemo();
        for (int i = 0; i < 10; i++) {
            new Thread(demo::m1).start();  // No lock in compiled code
        }
    }
}
```

**Mechanism**:  
- The JIT detects that `localObj` is never escaped (thread-local).  
- Eliminates `monitorenter`/`monitorexit` bytecodes, improving performance.  


#### **13.4.3 Lock Coarsening**
**Scenario**:  
When multiple adjacent synchronized blocks use the **same lock**, the JIT merges them into a single larger block to reduce lock/unlock overhead.

**Example**:  
```java
public class LockCoarsenDemo {
    static final Object lock = new Object();
    
    public static void main(String[] args) {
        new Thread(() -> {
            // Original: 3 separate sync blocks
            synchronized (lock) { System.out.println("111"); }
            synchronized (lock) { System.out.println("222"); }
            synchronized (lock) { System.out.println("333"); }
            
            // Optimized to: One merged block
            synchronized (lock) {
                System.out.println("111");
                System.out.println("222");
                System.out.println("333");
            }
        }).start();
    }
}
```

**Mechanism**:  
- Reduces **lock acquisition/release cycles** (CPU-intensive operations).  
- Applied only when blocks are **contiguous** and use the **same lock object**.  


#### **13.4.4 Key Takeaways**
| Optimization  | Trigger Condition                     | Benefit                          |
|---------------|---------------------------------------|----------------------------------|
| **Lock Elimination** | Lock object is thread-local           | Removes sync overhead entirely   |
| **Lock Coarsening** | Adjacent sync blocks with same lock   | Reduces lock/unlock frequency    |

**Impact**:  
- **Throughput**: Up to 10x faster for thread-confined locks.  
- **Latency**: Coarsening cuts sync overhead by ~50% in loop-heavy code.  

**Verification**:  
Use `-XX:+PrintAssembly` to observe optimized native code (requires HSDB).  

These optimizations demonstrate how the JVM intelligently balances **safety** and **performance** in concurrent code.

{pagebreak}